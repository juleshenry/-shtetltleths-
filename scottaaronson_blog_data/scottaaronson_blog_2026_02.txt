TITLE: “My Optimistic Vision for 2050”
URL: https://scottaaronson.blog/?m=202602
CONTENT:
The following are prepared remarks that I delivered by Zoom to a student group at my old stomping-grounds of MIT, and which I thought might interest others (even though much of it will be familiar to Shtetl-Optimized regulars).  The students asked me to share my “optimistic vision” for the year 2050, so I did my best to oblige.  A freewheeling discussion then followed, as a different freewheeling discussion can now follow in the comments section.

I was asked to share my optimistic vision for the future. The trouble is, optimistic visions for the future are not really my shtick!

It’s not that I’m a miserable, depressed person—I only sometimes am!  It’s just that, on a local level, I try to solve the problems in front of me, which have often been problems in computational complexity or quantum computing theory.

And then, on a global level, I worry about the terrifying problems of the world, such as climate change, nuclear war, and of course the resurgence of populist, authoritarian strongmen who’ve turned their backs on the Enlightenment and appeal to the basest instincts of humanity. I won’t name any names.

So then my optimistic vision is simply that we survive all this—“we” meaning the human race, but also meaning communities that I personally care about, like Americans, academics, scientists, and my extended family. We survive all of it so that we can reach the next crisis, the one where we don’t even know what it is yet.

But I get the sense that you wanted more optimism than that! Since I’ve spent 27 years working in quantum computing, the easiest thing for me to do would be to spin an optimistic story about how QC is going to make our lives so much better in 2050, by, I dunno, solving machine learning and optimization problems much faster, curing cancer, fixing global warming, whatever.

The good news is that there has been spectacular progress over the past couple years toward actually building a scalable QC. We now have two-qubit gates with 99.9% accuracy, close to the threshold where quantum error-correction becomes a net win. We can now do condensed-matter physics simulations that give us numbers that we don’t know how to get classically. I think it’s fair to say that all the key ideas and hardware building blocks for a fault-tolerant quantum computer are now in place, and what remains is “merely” the staggeringly hard engineering problem, which might take a few years, or a decade or more, but should eventually be solved.

The trouble for the optimistic vision is that the applications, where quantum algorithms outperform classical ones, have stubbornly remained pretty specialized.  In fact, the two biggest ones remain the two that we knew about in the 1990s:

Quantum simulation could help with designing better batteries, or solar cells, or high-temperature superconductors, or other materials, but the road from improved understanding to practical value is long and uncertain.  Meanwhile, breaking public-key cryptography could help various spy agencies and hackers and criminal syndicates, but it doesn’t obviously help the world.

The quantum speedups that we know outside those two categories—for example, for optimization and machine learning—tend to be either modest or specialized or speculative.

Honestly, the application of QC that excites me the most, by far, is just disproving all the people who said QC was impossible!

So much for QC then.

And so we come to the elephant in the room—the elephant in pretty much every room nowadays—which is AI. AI has now reached a place that exceeds the imaginations of many of the science-fiction writers of generations past—excelling not only at writing code and solving math competition problems but at depth of emotional understanding. Many of my friends are terrified of where this is leading us—and not in some remote future but in 5 or 10 or 20 years. I think they’re probably correct to be terrified. There’s an enormous range of possible outcomes on the table, including ones where the new superintelligences that we bring into being treat humans basically as humans treated the dodo bird, or the earlier hominids that used to share the earth with us.

But, within this range of outcomes, I think there are also some extremely good ones. Look, for millennia, people have prayed to God or gods for help, life, health, longevity, freedom, justice—and for millennia, God has famously been pretty slow to answer their prayers. A superintelligence that was aligned with human values would be nothing less than a God who did answer, who did deliver all those things, because we had created it to do so. Or for religious people, perhaps such an AI would be the means by which the old God was finally able to deliver all those things into the temporal world. These are the stakes here.

To switch metaphors, people sometimes describe the positive AI-enabled future as “luxury space communism.” AI would take care of all of our material needs, leaving us to seek value in our lives through family, friendships, competition, hobbies, humor, art, entertainment, or exploration. The super-AI would give us the freedom to pursue all those things, but would not give us the freedom to harm each other, to curtail each others’ freedoms, or to build a bad AI capable of overthrowing it. The super-AI would be a singleton, a monotheistic God or its emissary on earth.

Many people say that something would still be missing from this future. After all, we humans would no longer really be needed for anything—for building or advancing or defending civilization. To put a personal fine point on it, my students and colleagues and I wouldn’t needed any more to discover new scientific truths or to write about them. That would all be the AI’s job.

I agree that something would be lost here. But on the other hand, what fraction of us are needed right now for these things? Most humans already derive the meaning in their lives from family and community and enjoying art and music and food and things like that. So maybe the remaining fraction of us should just get over ourselves! On the whole, while this might not be the best future imaginable, I would accept it in a heartbeat given the realistic alternatives on offer. Thanks for listening.

================================================================================

TITLE: Nate Soares visiting UT Austin tomorrow!
URL: https://scottaaronson.blog/?m=202602
CONTENT:
This is just a quick announcement that I’ll be hosting Nate Soares—who coauthored the self-explanatorily titled If Anyone Builds It, Everyone Dies with Eliezer Yudkowsky—tomorrow (Tuesday) at 5PM at UT Austin, for a brief talk followed by what I’m sure will be an extremely lively Q&A about his book.  Anyone in the Austin area is welcome to join us.

================================================================================

TITLE: Luca Trevisan Award for Expository Work
URL: https://scottaaronson.blog/?m=202602
CONTENT:
Friend-of-the-blog Salil Vadhan has asked me to share the following.

The Trevisan Award for Expository Work is a new SIGACT award created in memory of Luca Trevisan (1971-2024), with a nomination deadline of April 10, 2026.

The award is intended to promote and recognize high-impact work expositing ideas and results from the Theory of Computation. The exposition can have various target audiences, e.g. people in this field, people in adjacent or remote academic fields, as well as the general public. The form of exposition can vary, and can include books, surveys, lectures, course materials, video, audio (e.g. podcasts), blogs and other media products. The award may be given to a single piece of work or a series produced over time. The award may be given to an individual, or a small group who together produced this expository work.

The awardee will receive USD$2000 (to be divided among the awardees if multiple), as well as travel support if needed to attend STOC, where the award will be presented. STOC’2026 is June 22-26 in Salt Lake City, Utah.

The endowment for this prize was initiated by a gift from Avi Wigderson, drawing on his Turing Award, and has been subsequently augmented by other individuals.

For more details see here.

================================================================================

TITLE: The time I didn’t meet Jeffrey Epstein
URL: https://scottaaronson.blog/?m=202602
CONTENT:
Last night, I was taken aback to discover that my name appears in the Epstein Files, in 26 different documents. This is despite the fact that I met Jeffrey Epstein a grand total of zero times, and had zero email or any other contact with him … which is more (less) than some of my colleagues can say.

The bulk of the correspondence involves Epstein wanting to arrange a meeting with me and Seth Lloyd back in 2010, via an intermediary named Charles Harper, about funding a research project on “Cryptography in Nature.”

Searching my inbox, it turns out that this Charles Harper did contact me in May 2010, and I then met him at S&S Deli in Cambridge (plausible, although I have zero recollections of this meeting—only of the deli).  Harper then sent me a detailed followup email about his proposed Cryptography in Nature project, naming Jeffrey Epstein for the first time as the project’s funder, and adding: “perhaps you will know Jeffrey and his background and situation.”

For whatever reason, I forwarded this email to my parents, brother, and then-fiancee Dana. My brother then found and shared a news article about Epstein’s prostitution conviction, adding to a different article that I had found and shared. (At that time, like many others, I’d probably vaguely heard of Epstein, but he didn’t have 0.1% the infamy that he has now.) Then my mom wrote the following: “be careful not to get sucked up in the slime-machine going on here! Since you don’t care that much about money, they can’t buy you at least.”

It appears from emails that Charles Harper tried again later that summer to arrange a meeting between me and Epstein, but that I took my mom’s advice and largely blew him off, and no such meeting ever happened. Amazingly, I then forgot entirely that any of this had occurred until last night.  By way of explanation, some business/finance dude trying to interest me in half-baked ideas involving quantum, AI, cryptography, etc., often dangling the prospect of funding for my students and postdocs, shows up in my life like every month.  Most of their world-changing initiatives go nowhere for one reason or another.  There really wasn’t much reason to think further about this, until Epstein had become history’s most notorious sex criminal, which (again) wouldn’t happen until years later, after I’d forgotten.

It gets better, though.  In the Epstein Files, one also finds a November 2010 letter from Charles Harper to Epstein about organizing a conference on the same Cryptography in Nature topic, which includes the following idea about me:

Scott Aaronson was born on May 21st, 1981. He will be 30 in 2011. The conference could follow a theme of: “hurry to think together with Scott Aaronson while he is still in his 20s and not yet a pitiful over-the-hill geezer in his 30s.” This offers another nice opportunity for celebration.

I see no indication that any such conference ever happened; in any case, I didn’t get invited to one!

On my Facebook, some friends are joking that “it tracks that someone into teenage girls might think Scott Aaronson was a hot property in his nubile 20s, who would get old and boring in his 30s”—and that maybe Epstein was less sexist about such matters than everyone assumes.  I replied that I wished I could say the proposition that I’d gradually get slower and more senile through the 2010s and 2020s was entirely false.

But the best comment was that I’ve been incredibly lucky to have such an astute family.  If only Bill Gates and Larry Summers had had my mom to go to for advice, they could’ve saved themselves a lot of grief.

================================================================================

